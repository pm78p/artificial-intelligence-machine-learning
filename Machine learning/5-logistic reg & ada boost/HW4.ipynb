{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "HW4_97106143.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fewer-fields"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math"
      ],
      "id": "fewer-fields",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expensive-nursing",
        "outputId": "b89acec6-c7d7-45db-893a-9fb9d16d4a48"
      },
      "source": [
        "data = pd.read_csv(\"transfusion.data\", names = [\"last donation\", \"total donations frequency\", \"total blood donation\", \"first donation\", \"donation in March 2007\"], header = 0)\n",
        "data = data.sample(frac = 1).reset_index(drop = True)\n",
        "X = data[[\"last donation\", \"total donations frequency\", \"total blood donation\", \"first donation\"]]\n",
        "Y = data[\"donation in March 2007\"]\n",
        "data"
      ],
      "id": "expensive-nursing",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>last donation</th>\n",
              "      <th>total donations frequency</th>\n",
              "      <th>total blood donation</th>\n",
              "      <th>first donation</th>\n",
              "      <th>donation in March 2007</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>8</td>\n",
              "      <td>2000</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>750</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>750</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>5250</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>250</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1250</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1250</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>750</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>748 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     last donation  total donations frequency  total blood donation  \\\n",
              "0                4                          2                   500   \n",
              "1               18                          8                  2000   \n",
              "2               14                          3                   750   \n",
              "3               14                          3                   750   \n",
              "4                2                          2                   500   \n",
              "..             ...                        ...                   ...   \n",
              "743              3                         21                  5250   \n",
              "744             14                          1                   250   \n",
              "745              4                          5                  1250   \n",
              "746              4                          5                  1250   \n",
              "747             11                          3                   750   \n",
              "\n",
              "     first donation  donation in March 2007  \n",
              "0                41                       0  \n",
              "1                95                       0  \n",
              "2                35                       0  \n",
              "3                41                       0  \n",
              "4                 2                       0  \n",
              "..              ...                     ...  \n",
              "743              42                       1  \n",
              "744              14                       0  \n",
              "745              11                       1  \n",
              "746              58                       0  \n",
              "747              37                       0  \n",
              "\n",
              "[748 rows x 5 columns]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigger-death"
      },
      "source": [
        "## Data preparation\n",
        "<br>\n",
        "<div style=\"font-size:20px;direction:LTR\">\n",
        "To learn the decision tree we need the data to be discrete. In this section we prepare the required data of the decision tree.\n",
        "The dis and sub_dis functions are for this purpose. To disassemble the data, we first find the best point that gives us the most information gain. The data is then divided into two parts. Now, to reach more categories, we repeat the same process on the two obtained categories.\n",
        "    The cal_antropy function also calculates a batch by taking the entropy label<br>\n",
        "    </div>"
      ],
      "id": "bigger-death"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "united-delight",
        "outputId": "f4f24b64-e4d6-44a3-fad1-18d8d1b78a94"
      },
      "source": [
        "def cal_antropy(labels):\n",
        "    n = len(labels)\n",
        "    temp = 0\n",
        "    for label in labels:\n",
        "        if label == 1:\n",
        "            temp += 1\n",
        "    try:\n",
        "        antropy = -(temp / n * math.log(temp / n, 2) + (n - temp) / n * math.log((n - temp) / n, 2))\n",
        "    except:\n",
        "        antropy = 0\n",
        "    return antropy\n",
        "\n",
        "def dis(data, feature, subset_number):\n",
        "    df = data.sort_values(by=feature, ascending=True)\n",
        "    Y = list(df[\"donation in March 2007\"])\n",
        "    X = list(df[feature])\n",
        "    n = len(Y)\n",
        "    \n",
        "    if (subset_number == 0 or not X):\n",
        "        return []\n",
        "    \n",
        "    \n",
        "    candidates = []\n",
        "    for i in range(1, len(Y)):\n",
        "        if Y[i] != Y[i-1]:\n",
        "            candidates.append(i)\n",
        "    \n",
        "    antropies = [(cal_antropy(Y[:i]) * i + cal_antropy(Y[i:]) * (n - i)) / n for i in candidates]\n",
        "    best_threshold_indice = candidates[np.argmin(antropies)]\n",
        "    best_threshold = X[best_threshold_indice]\n",
        "    \n",
        "    left_subset_number = math.floor(subset_number / 2)\n",
        "    \n",
        "    left_thresholds = sub_dis(X[:best_threshold_indice], Y[:best_threshold_indice], left_subset_number)\n",
        "    right_thresholds = sub_dis(X[best_threshold_indice:], Y[best_threshold_indice:], subset_number - left_subset_number)\n",
        "    \n",
        "#     print(left_thresholds + [best_threshold] + right_thresholds + [X[-1]])\n",
        "    boundries = sorted(list(set(left_thresholds + [best_threshold] + right_thresholds + [X[-1]])))\n",
        "    \n",
        "    X = data[feature]\n",
        "    for i in range(len(X)):\n",
        "        for j in range(len(boundries)):\n",
        "            if X[i] <= boundries[j]:\n",
        "                X[i] = j\n",
        "                break\n",
        "    \n",
        "#     return boundries\n",
        "\n",
        "    \n",
        "    \n",
        "def sub_dis(X, Y, subset_number):\n",
        "    n = len(Y)\n",
        "    if (subset_number == 0 or not X):\n",
        "        return []\n",
        "    if (subset_number == 1):\n",
        "        return []\n",
        "    \n",
        "    \n",
        "    candidates = []\n",
        "    for i in range(1, len(Y)):\n",
        "        if Y[i] != Y[i-1]:\n",
        "            candidates.append(i)\n",
        "            \n",
        "    if (not candidates):\n",
        "        return [X[-1]]\n",
        "            \n",
        "    antropies = [(cal_antropy(Y[:i]) * i + cal_antropy(Y[i:]) * (n - i)) / n for i in candidates]\n",
        "    best_threshold_indice = candidates[np.argmin(antropies)]\n",
        "    best_threshold = X[best_threshold_indice]\n",
        "    \n",
        "    left_subset_number = math.floor(subset_number / 2)\n",
        "    \n",
        "    left_thresholds = sub_dis(X[:best_threshold_indice], Y[:best_threshold_indice], left_subset_number)\n",
        "    right_thresholds = sub_dis(X[best_threshold_indice:], Y[best_threshold_indice:], subset_number - left_subset_number)\n",
        "    \n",
        "    \n",
        "    return left_thresholds + [best_threshold] + right_thresholds \n",
        "    \n",
        "\n",
        "# print(Y[5:])\n",
        "data_discrite = data.copy()\n",
        "dis(data_discrite, \"last donation\", 6)\n",
        "dis(data_discrite, \"total donations frequency\", 6)\n",
        "dis(data_discrite, \"total blood donation\", 6)\n",
        "dis(data_discrite, \"first donation\", 6)\n",
        "\n",
        "classes_num = {\"last donation\": max(data_discrite[\"last donation\"]) + 1,\n",
        "                            \"total donations frequency\" : max(data_discrite[\"total donations frequency\"]) + 1,\n",
        "                            \"total blood donation\" : max(data_discrite[\"total blood donation\"]) + 1,\n",
        "                            \"first donation\" : max(data_discrite[\"first donation\"]) + 1}\n",
        "\n",
        "\n",
        "X_discrite = data_discrite[[\"last donation\", \"total donations frequency\", \"total blood donation\", \"first donation\"]]\n",
        "Y_discrite = data_discrite[\"donation in March 2007\"]\n",
        "border = int(len(X_discrite) * 4 / 5)\n",
        "X_discrite_train, Y_discrite_train = X_discrite[:border], np.array(Y[:border])\n",
        "X_discrite_test, Y_discrite_test = X_discrite[border:], np.array(Y[border:])\n",
        "\n",
        "X_discrite_test = X_discrite_test.reset_index(drop = True)\n",
        "\n",
        "data_discrite\n"
      ],
      "id": "united-delight",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>last donation</th>\n",
              "      <th>total donations frequency</th>\n",
              "      <th>total blood donation</th>\n",
              "      <th>first donation</th>\n",
              "      <th>donation in March 2007</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>748 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     last donation  total donations frequency  total blood donation  \\\n",
              "0                0                          1                     1   \n",
              "1                2                          2                     2   \n",
              "2                2                          1                     1   \n",
              "3                2                          1                     1   \n",
              "4                0                          1                     1   \n",
              "..             ...                        ...                   ...   \n",
              "743              0                          3                     3   \n",
              "744              2                          0                     0   \n",
              "745              0                          2                     2   \n",
              "746              0                          2                     2   \n",
              "747              1                          1                     1   \n",
              "\n",
              "     first donation  donation in March 2007  \n",
              "0                 1                       0  \n",
              "1                 2                       0  \n",
              "2                 1                       0  \n",
              "3                 1                       0  \n",
              "4                 0                       0  \n",
              "..              ...                     ...  \n",
              "743               1                       1  \n",
              "744               1                       0  \n",
              "745               1                       1  \n",
              "746               2                       0  \n",
              "747               1                       0  \n",
              "\n",
              "[748 rows x 5 columns]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "white-stuff"
      },
      "source": [
        "<div style=\"font-size:20px;direction:LTR\"> In this section we prepare the data for logistic regression. This requires the data to be normalized. In the following section, we have scaled the data very easily between -1 and 1, by dividing each column by the maximum data of that column and then minus 1.\n",
        "    </div>"
      ],
      "id": "white-stuff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "specific-combination"
      },
      "source": [
        "X_logistic = pd.DataFrame(data = {\"last donation\": 2 * X[\"last donation\"] / max(X[\"last donation\"] - 1),\n",
        "                            \"total donations frequency\" : 2 * X[\"total donations frequency\"] / max(X[\"total donations frequency\"] - 1),\n",
        "                            \"total blood donation\" : 2 * X[\"total blood donation\"] / max(X[\"total blood donation\"] - 1),\n",
        "                            \"first donation\" : 2 * X[\"first donation\"] / max(X[\"first donation\"] - 1)})\n",
        "X_logistic = np.c_[X_logistic, np.ones((len(X),1))]\n",
        "border = int(len(X_logistic) * 4 / 5)\n",
        "X_logistic_train, Y_logistic_train = X_logistic[:border], np.array(Y[:border])\n",
        "X_logistic_test, Y_logistic_test = X_logistic[border:], np.array(Y[border:])"
      ],
      "id": "specific-combination",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irish-ordering"
      },
      "source": [
        "## 1 \n",
        "## logistic regression model\n",
        "<br>\n",
        "<div style=\"font-size:20px;direction:LTR\">\n",
        " Now we train the logistic regression model. Because we do not have a closed form for the answer, the gradient_descent function finds the answer for us. Also, the related parameters can be passed to the model. We have two other functions to change the model threshold (for use in part 3) as well as predictions\n",
        "    </div>"
      ],
      "id": "irish-ordering"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sixth-coating"
      },
      "source": [
        "class logistic_regression:\n",
        "    \n",
        "    def __init__(self, X, Y, learning_rate = 0.01, iterations = 300):\n",
        "        self.W = logistic_regression.gradient_descent(X, Y, learning_rate = learning_rate, iterations = iterations)\n",
        "        self.threshold = 0.5\n",
        "        \n",
        "    def set_threshold(self, threshold):\n",
        "        self.threshold = threshold\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return 1 if 1 / (1 + math.exp(-np.dot(self.W, X))) >= self.threshold else 0\n",
        "    \n",
        "    @staticmethod   \n",
        "    def gradient_descent(X,Y,W = [0, 0, 0, 0, 0], learning_rate=0.01, iterations=300):\n",
        "        for it in range(iterations):\n",
        "            sigmoids = [1 / (1 + math.exp(-np.dot(W, X[i]))) for i in range(len(X))]\n",
        "\n",
        "            E = [(sigmoids[i] - Y[i]) * X[i] for i in range(len(sigmoids))]\n",
        "            W = W - learning_rate * sum([(sigmoids[i] - Y[i]) * X[i] for i in range(len(sigmoids))])\n",
        "        return W\n",
        "    \n",
        "    \n",
        "\n",
        "logistic_regressor = logistic_regression(X_logistic_train, Y_logistic_train)"
      ],
      "id": "sixth-coating",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "disciplinary-headquarters"
      },
      "source": [
        "## Decision Tree\n",
        "<br>\n",
        "<div style=\"font-size:20px;direction:LTR\">\n",
        "In this part, we train the decision tree. On the calculation of entropy and information gain, we have two functions cal_antropy and cal_information_gain, the first of which receives the entropy of the input data and the second with the received data and also a feature that determines how much information gain we have if we divide the data based on the received feature.\n",
        "    We also have a node class that actually corresponds to our nodes in the decision tree. We also make the decision tree recursively. Using the best_feature function, we find the best feature in terms of information_gain, and then using the sub_nodes function, we find the bottom nodes of the node we are in. Now, using the make_sub_tree function, we extend each of the nodes under its nodes.\n",
        "    </div>"
      ],
      "id": "disciplinary-headquarters"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pretty-frontier"
      },
      "source": [
        "def cal_antropy(labels):\n",
        "    n = len(labels)\n",
        "    temp = 0\n",
        "    for label in labels:\n",
        "        if label == 1:\n",
        "            temp += 1\n",
        "    try:\n",
        "        antropy = -(temp / n * math.log(temp / n, 2) + (n - temp) / n * math.log((n - temp) / n, 2))\n",
        "    except:\n",
        "        antropy = 0\n",
        "    return antropy\n",
        "\n",
        "def cal_information_gain(X, Y, feature):\n",
        "    n = len(Y)\n",
        "    current_antropy = cal_antropy(Y)\n",
        "    class_num = classes_num[feature]\n",
        "    information_gain = []\n",
        "    subset_lengths = []\n",
        "    for i in range(class_num):\n",
        "        indices = X[feature] == i\n",
        "        new_Y = Y[indices]\n",
        "        information_gain.append((current_antropy - cal_antropy(new_Y)) * len(indices) / n)\n",
        "    return sum(information_gain)\n",
        "\n",
        "class decision_tree:\n",
        "    \n",
        "    def __init__(self, X, Y, features):\n",
        "        self.threshold = 0.5\n",
        "        self.root = tree_node(X, Y)\n",
        "        feature = decision_tree.best_feature(self.root.X, self.root.Y, features)\n",
        "        self.root.sub_nodes = decision_tree.sub_nodes(self.root, feature)\n",
        "        self.root.feature = feature\n",
        "        features_left = features.copy()\n",
        "        features_left.remove(feature)\n",
        "        for sub_node in self.root.sub_nodes:\n",
        "            decision_tree.make_sub_tree(sub_node, features_left)\n",
        "        return\n",
        "        \n",
        "    def set_threshold(self, threshold):\n",
        "        self.threshold = threshold\n",
        "        \n",
        "    @staticmethod\n",
        "    def make_sub_tree(node, features):\n",
        "        if features:\n",
        "            if(len(node.X) != 0):\n",
        "                feature = decision_tree.best_feature(node.X, node.Y, features)\n",
        "                node.sub_nodes = decision_tree.sub_nodes(node, feature)\n",
        "                node.feature = feature\n",
        "                features_left = features.copy()\n",
        "                features_left.remove(feature)\n",
        "                for sub_node in node.sub_nodes:\n",
        "                        decision_tree.make_sub_tree(sub_node, features_left)\n",
        "        \n",
        "    @staticmethod\n",
        "    def best_feature(X, Y, features):\n",
        "        feature = features[0]\n",
        "        information_gain = cal_information_gain(X, Y, feature)\n",
        "        for i in range(1, len(features)):\n",
        "            temp_gain = cal_information_gain(X, Y, features[i])\n",
        "            if temp_gain > information_gain:\n",
        "                information_gain = temp_gain\n",
        "                feature = features[i]\n",
        "        return feature\n",
        "    \n",
        "    @staticmethod\n",
        "    def sub_nodes(node, feature):\n",
        "        X = node.X\n",
        "        Y = node.Y\n",
        "        class_num = classes_num[feature]\n",
        "        sub_X = []\n",
        "        sub_Y = []\n",
        "        sub_nodes = []\n",
        "        for i in range(class_num):\n",
        "            indices = X[feature] == i\n",
        "            sub_nodes.append(tree_node(X[indices], Y[indices]))\n",
        "        return sub_nodes\n",
        "    \n",
        "    def predict(self, x):\n",
        "        node = self.root\n",
        "        while (True):\n",
        "            if len(node.X) == 0 or not node.sub_nodes:\n",
        "                break\n",
        "            feature = node.feature\n",
        "            node = node.sub_nodes[x[feature]]\n",
        "        n = len(node.X)\n",
        "        if n == 0:\n",
        "            return 0\n",
        "        positives_prob = sum(node.Y == 1) / n\n",
        "        return 1 if positives_prob >= self.threshold else 0\n",
        "\n",
        "class tree_node:\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.sub_nodes = []\n",
        "        \n",
        "DT = decision_tree(X_discrite_train, Y_discrite_train, ['last donation', 'total donations frequency', 'total blood donation', 'first donation'])\n"
      ],
      "id": "pretty-frontier",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "provincial-consumption"
      },
      "source": [
        "## metrics\n",
        "<div style=\"font-size:20px;direction:RTL\">\n",
        "In this section, we first implement the cal_metrics pan, which calculates and returns the precision recall f1-score by taking the main list of labels as well as the forecast list.\n",
        "    After that, for each of the logistic and decision tree models, we performed the prediction on the test data and printed the result.\n",
        "    </div>"
      ],
      "id": "provincial-consumption"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "continued-exhibition",
        "outputId": "f1a061ba-4374-4c21-ba5a-2f99c5d67d07"
      },
      "source": [
        "def cal_metrics(Y_pred, Y_act):\n",
        "    tp = fp = tn = fn = 0\n",
        "    \n",
        "    for i in range(len(Y_pred)):\n",
        "        if(Y_pred[i] == 1 and Y_act[i] == 1):\n",
        "            tp += 1\n",
        "        elif(Y_pred[i] == 0 and Y_act[i] == 0):\n",
        "            tn += 1\n",
        "        elif(Y_pred[i] == 0 and Y_act[i] == 1):\n",
        "            fn += 1\n",
        "        else:\n",
        "            fp += 1\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    try:\n",
        "        precision = tp / (tp + fp)\n",
        "    except:\n",
        "        precision = 0\n",
        "    recall = tp / (tp + fn)\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except:\n",
        "        f1 = 0\n",
        "    return accuracy, recall, f1\n",
        "   \n",
        "\n",
        "# logistic regressor prediction\n",
        "\n",
        "# logistic_regressor.set_threshold(0.5)\n",
        "y_pred = []\n",
        "for i in range(len(X_logistic_test)):\n",
        "    pred = logistic_regressor.predict(X_logistic_test[i])\n",
        "    y_pred.append(pred)\n",
        "logisitic_accuracy, logisitic_recall, logisitic_f1 = cal_metrics(y_pred, Y_logistic_test)\n",
        "print('logistic regression: \\naccuracy = {} \\t recall = {} \\t f1 = {}'.format(logisitic_accuracy, logisitic_recall, logisitic_f1))\n",
        "\n",
        "# decision tree prediction\n",
        "\n",
        "# DT.set_threshold(0.5)\n",
        "y_pred = []\n",
        "for i in range(len(X_discrite_test)):\n",
        "    pred = DT.predict(X_discrite_test.iloc[i])\n",
        "    y_pred.append(pred)\n",
        "DT_accuracy, DT_recall, DT_f1 = cal_metrics(y_pred, Y_discrite_test)\n",
        "print('logistic regression: \\naccuracy = {} \\t recall = {} \\t f1 = {}'.format(DT_accuracy, DT_recall, DT_f1))\n"
      ],
      "id": "continued-exhibition",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic regression: \n",
            "accuracy = 0.7733333333333333 \t recall = 0.08333333333333333 \t f1 = 0.15\n",
            "logistic regression: \n",
            "accuracy = 0.8333333333333334 \t recall = 0.5833333333333334 \t f1 = 0.626865671641791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "durable-converter"
      },
      "source": [
        "## 2"
      ],
      "id": "durable-converter"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "included-revelation"
      },
      "source": [
        "<p style=\"font-size: 20px;direction:LTR\" >The reason for the low recall and f1-score is that about 75% of our data belongs to class 0, which makes our models can not separate classes 1 well. Also, except for the first feature of the data, the rest of them do not have a specific information gain, and this makes the data not separate in general. In order to have a better recall, which also leads to a better f1-score, the threshold value can be reduced to less than 0.5 so that the models have more flexibility for separating Class 1 data.\n",
        "<br>\n",
        "The following figure shows the class diagram based on the first feature, which shows the amount of class separation. The rest of the feature charts do not show anything special.\n",
        "</p>"
      ],
      "id": "included-revelation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "editorial-reunion",
        "outputId": "909f0c1d-2eb5-47e7-aeb1-0f48d867c1b8"
      },
      "source": [
        "# data = pd.read_csv(\"transfusion.data\", names = [\"last donation\", \"total donations frequency\", \"total blood donation\", \"first donation\", \"donation in March 2007\"], header = 0)\n",
        "\n",
        "# sns.scatterplot(data=data, x='last donation', y='donation in March 2007', hue='donation in March 2007');\n",
        "sns.jointplot(x=data[\"last donation\"], y=data[\"donation in March 2007\"], kind=\"hex\", color=\"#4CB391\");"
      ],
      "id": "editorial-reunion",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eastern-married"
      },
      "source": [
        "## 3\n",
        "<div style=\"font-size:20px;direction:ltr\">\n",
        "In this section, we first use the set_threshold function, which was coded in the first part, to change the threshold value in such a way that we achieve acceptable results. Because about 20 to 25 percent of the data is from Class 1, it seems that threshold Deniz is slightly better with a little more.\n",
        "    </div>"
      ],
      "id": "eastern-married"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sharp-raising",
        "outputId": "57d9ff87-ba68-442b-cbce-29aa5d924c9f"
      },
      "source": [
        "# logistic regressor prediction\n",
        "logistic_regressor.set_threshold(0.3)\n",
        "\n",
        "y_pred = []\n",
        "for i in range(len(X_logistic_test)):\n",
        "    pred = logistic_regressor.predict(X_logistic_test[i])\n",
        "    y_pred.append(pred) \n",
        "logisitic_accuracy, logisitic_recall, logisitic_f1 = cal_metrics(y_pred, Y_logistic_test)\n",
        "print('logistic regression: \\naccuracy = {} \\t recall = {} \\t f1 = {}'.format(logisitic_accuracy, logisitic_recall, logisitic_f1))\n",
        "\n",
        "\n",
        "\n",
        "# decision tree prediction\n",
        "DT.set_threshold(0.25)\n",
        "\n",
        "y_pred = []\n",
        "for i in range(len(X_discrite_test)):\n",
        "    pred = DT.predict(X_discrite_test.iloc[i])\n",
        "    y_pred.append(pred)\n",
        "DT_accuracy, DT_recall, DT_f1 = cal_metrics(y_pred, Y_discrite_test)\n",
        "print('Decision Tree: \\naccuracy = {} \\t recall = {} \\t f1 = {}'.format(DT_accuracy, DT_recall, DT_f1))\n"
      ],
      "id": "sharp-raising",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic regression: \n",
            "accuracy = 0.6933333333333334 \t recall = 0.7222222222222222 \t f1 = 0.5306122448979592\n",
            "Decision Tree: \n",
            "accuracy = 0.7533333333333333 \t recall = 0.7222222222222222 \t f1 = 0.5842696629213483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "first-recruitment"
      },
      "source": [
        "<p style=\"font-size: 20px;direction:ltr\"> As you can see, we were able to increase the recall and f1-score at the expense of reducing the precision, but further reducing the threshold causes a very large reduction in precision, which is not desirable.</p>"
      ],
      "id": "first-recruitment"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valuable-fisher"
      },
      "source": [
        "## 4\n",
        "<p style=\"font-size: 20px;direction:ltr\">\n",
        "In this section, we want to train the adaBoost classifier. The adaBoost_classifier class is for this purpose. Its inputs are our data and m, which specifies the number of stumps. In the train_model function, we train the model. In this way, first the weight of all data is equal. Then, in each step, using the best_stump function, we find the best tree with a depth of one that has the least weight error. In fact, we choose the best between 4 different stumps, each made on one of the X features. After selecting the best stump, we use it to get its alpha value and then update the data weight.\n",
        "    The set_threshold function is also used to change the thresholds of the stumps, which will be used in the next section\n",
        "    </p>"
      ],
      "id": "valuable-fisher"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "promotional-discussion"
      },
      "source": [
        "class adaBoost_classifier:\n",
        "    def __init__(self, X, Y, m):\n",
        "        self.train_model(X, Y, m)\n",
        "        self.thresholds = 0.5\n",
        "        \n",
        "    def train_model(self, X, Y, m):\n",
        "        n = len(X)\n",
        "        W = [1/n] * n\n",
        "        stumps = []\n",
        "        alphas = []\n",
        "        for i in range(m):\n",
        "            stump, stump_error = adaBoost_classifier.best_stump(X, Y, W)\n",
        "            stumps.append(stump)\n",
        "            epsilon = stump_error / sum(W)\n",
        "            alphas.append(math.log((1 - epsilon) / epsilon))\n",
        "            for j in range(len(W)):\n",
        "                if (stump.predict(X.iloc[j]) != Y[j]):\n",
        "                    W[j] = W[j] * math.exp(alphas[i])\n",
        "        self.stumps = stumps\n",
        "        self.alphas = alphas\n",
        "    \n",
        "    def set_threshold(self, threshold):\n",
        "        for stump in self.stumps:\n",
        "            stump.set_threshold(threshold)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        pred = sum([self.alphas[i] * self.stumps[i].predict(X) for i in range(len(self.alphas))])\n",
        "        return 1 if pred > 0.5 else 0\n",
        "        \n",
        "    @staticmethod\n",
        "    def best_stump(X, Y, W):\n",
        "        features = X.columns\n",
        "        dt = decision_tree(X, Y, [features[0]])\n",
        "#         dt.set_threshold(0.3)\n",
        "        error = adaBoost_classifier.cal_error(dt, X, Y, W)\n",
        "        for i in range(1, len(features)):\n",
        "            temp_dt = decision_tree(X, Y, [features[i]])\n",
        "#             temp_dt.set_threshold(0.3)\n",
        "            temp_error = adaBoost_classifier.cal_error(temp_dt, X, Y, W)\n",
        "            if temp_error < error:\n",
        "                error = temp_error\n",
        "                dt = temp_dt\n",
        "        return dt, error\n",
        "        \n",
        "        \n",
        "    @staticmethod\n",
        "    def cal_error(DT, X, Y, W):\n",
        "        error = 0\n",
        "        for i in range(len(X)):\n",
        "            pred = DT.predict(X.iloc[i])\n",
        "            if pred != Y[i]:\n",
        "                error += W[i]\n",
        "        return error\n",
        "                \n",
        "adaBoost_Cl = adaBoost_classifier(X_discrite_train, Y_discrite_train, 4)"
      ],
      "id": "promotional-discussion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "editorial-orchestra"
      },
      "source": [
        "<p style=\"font-size: 20px;direction:ltr\">\n",
        "In this section, we have obtained the model prediction test and printed its metrics\n",
        "    </p>"
      ],
      "id": "editorial-orchestra"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boolean-prairie",
        "outputId": "93d8b041-b7fa-4742-ffad-8826789976ea"
      },
      "source": [
        "# adaBoost_Cl.set_threshold(0.5)\n",
        "\n",
        "y_pred = []\n",
        "for i in range(len(X_discrite_test)):\n",
        "    pred = adaBoost_Cl.predict(X_discrite_test.iloc[i])\n",
        "    y_pred.append(pred)\n",
        "    \n",
        "DT_accuracy, DT_recall, DT_f1 = cal_metrics(y_pred, Y_discrite_test)\n",
        "print('adaBoost: \\naccuracy = {} \\t recall = {} \\t f1 = {}'.format(DT_accuracy, DT_recall, DT_f1))\n",
        "# adaBoost_Cl.predict(X_discrite_train.iloc[10])"
      ],
      "id": "boolean-prairie",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adaBoost: \n",
            "accuracy = 0.76 \t recall = 0.027777777777777776 \t f1 = 0.05263157894736842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unauthorized-subscription"
      },
      "source": [
        "<p style=\"font-size:20px;direction:ltr\"> As can be seen, due to the large amount of Class 0 data, our category estimates almost all points in Class 0, causing the recall and f1-score to be very low. Now we solve this problem by changing the threshold of the used decision trees\n",
        "    </p>"
      ],
      "id": "unauthorized-subscription"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "specific-beaver",
        "outputId": "2941b6e0-ac77-4194-e4bd-4530045e3363"
      },
      "source": [
        "adaBoost_Cl.set_threshold(0.3)\n",
        "\n",
        "y_pred = []\n",
        "for i in range(len(X_discrite_test)):\n",
        "    pred = adaBoost_Cl.predict(X_discrite_test.iloc[i])\n",
        "    y_pred.append(pred)\n",
        "    \n",
        "DT_accuracy, DT_recall, DT_f1 = cal_metrics(y_pred, Y_discrite_test)\n",
        "print('adaBoost: \\naccuracy = {} \\t recall = {} \\t f1 = {}'.format(DT_accuracy, DT_recall, DT_f1))"
      ],
      "id": "specific-beaver",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adaBoost: \n",
            "accuracy = 0.64 \t recall = 0.7222222222222222 \t f1 = 0.490566037735849\n"
          ]
        }
      ]
    }
  ]
}